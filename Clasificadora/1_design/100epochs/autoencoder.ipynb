{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abccb29d",
   "metadata": {},
   "source": [
    "# Entrenamiento de Autoencoder y guardado de parámetros\n",
    "\n",
    "En este notebook se entrena un autoencoder convolucional sobre FashionMNIST, se registran las pérdidas de entrenamiento y validación y se guardan los parámetros del modelo para luego ser reutilizados desde `train.ipynb` en una red clasificadora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53f9708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (0.19.1)\n",
      "Requirement already satisfied: torchaudio in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: torchviz in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (0.0.3)\n",
      "Requirement already satisfied: jinja2 in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: fsspec in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: sympy in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: triton==3.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.13\" in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: filelock in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: networkx in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: numpy in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: graphviz in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from torchviz) (0.20.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (12.9.86)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/usuario/Documents/RN-FinalProyect/venv/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# 1) Instalación de librerías requeridas\n",
    "!pip3 install torch torchvision torchaudio torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f3767bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Importar librerías necesarias\n",
    "# 1.1)\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1.3)\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "#from torchviz import make_dot\n",
    "\n",
    "# Configuración de dispositivo\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7934f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1)\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "root_data = \"MNIST_data/\"\n",
    "# Download and load the full training data\n",
    "full_dataset = datasets.FashionMNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "\n",
    "# Split the full dataset into train and validation sets of (almost) equal size\n",
    "train_size = len(full_dataset) // 2\n",
    "valid_size = len(full_dataset) - train_size\n",
    "train_set_orig_autoencoder, valid_set_orig_autoencoder = random_split(full_dataset, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6b356e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2.2)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "figure = plt.figure()\n",
    "cols,rows = 3,3\n",
    "for i in range(1,cols*rows+1):\n",
    "    j = torch.randint(len(train_set_orig_autoencoder),size=(1,)).item() # Los números aleatorios tambien se pueden generar desde pytorch. Util para trabajar en la GPU.\n",
    "    image,label = train_set_orig_autoencoder[j]\n",
    "    figure.add_subplot(rows,cols,i)\n",
    "    #plt.title(labels_names[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image.squeeze(),cmap=\"Greys_r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d22587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1)\n",
    "# Creamos una subclase de la clase Dataset que nos sirva para generar lotes de ejemplos que puedan usarse para entrenar un autoencoder\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset=dataset\n",
    "    # Redefinimos el método .__len__()\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    # Redefinimos el método .__getitem__()\n",
    "    def __getitem__(self,i):\n",
    "        image,label=self.dataset[i]\n",
    "        input  = image\n",
    "        output = image #torch.flatten(image) # retornamos la imagen como salida\n",
    "        return input,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a9f80a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño train_set: 30000\n",
      "Tamaño valid_set: 30000\n"
     ]
    }
   ],
   "source": [
    "# 3.2)\n",
    "# Convertimos FashionMNIST Dataset a CustomDataset\n",
    "train_set_autoencoder = CustomDataset(train_set_orig_autoencoder)\n",
    "valid_set_autoencoder = CustomDataset(valid_set_orig_autoencoder)\n",
    "\n",
    "print(f\"Tamaño train_set: {len(train_set_autoencoder)}\")\n",
    "print(f\"Tamaño valid_set: {len(valid_set_autoencoder)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e78d452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Definir modelo Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"Experimento 1: Autoencoder convolucional básico\"\"\"\n",
    "\n",
    "    def __init__(self, dropout = 0.15):\n",
    "        super().__init__()\n",
    "        # Encoder: (1, 28, 28) -> (32, 14, 14) -> (64, 7, 7)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),  # (1, 28, 28) -> (32, 28, 28)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (32, 28, 28) -> (32, 14, 14)\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),  # (32, 14, 14) -> (64, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (64, 14, 14) -> (64, 7, 7)\n",
    "        )\n",
    "\n",
    "        # Decoder: (64, 7, 7) -> (32, 14, 14) -> (1, 28, 28)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=2, stride=2),  # (64, 7, 7) -> (32, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=1, kernel_size=2, stride=2),  # (32, 14, 14) -> (1, 28, 28)\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49fec747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0.15, inplace=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): ConvTranspose2d(32, 1, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Crear instancia de modelo\n",
    "p_dropout = 0.15\n",
    "autoencoder = Autoencoder(dropout=p_dropout).to(device)\n",
    "print(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d7ea4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1)\n",
    "# Definimos la función de entrenamiento\n",
    "def train_loop(dataloader, model, loss_fn, optimizer,verbose=True):\n",
    "    # Activamos la maquinaria de entrenamiento del modelo\n",
    "    model.train()\n",
    "    # Definimos ciertas constantes\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    sum_loss = 0\n",
    "    sum_samples = 0\n",
    "    # Movemos el modelo a la GPU si es que está disponible\n",
    "    model = model.to(device)\n",
    "    #Iteramos sobre lotes (batchs)\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        # Copiamos las entradas y salidas al dispositvo de trabajo si es que está disponible\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        batch_size = len(X)\n",
    "        sum_samples += batch_size\n",
    "        # Calculamos la predicción del modelo y la correspondiente función de pérdida\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred,y)\n",
    "        # Backpropagamos usando el optimizaor provisto\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Calculamos la pérdida promedio del batch y lo agregamos a una suma correspondiente\n",
    "        sum_loss += loss.item() * batch_size\n",
    "        # Reportamos el progreso\n",
    "        if batch % (num_batches/10) == 0 and verbose:\n",
    "            current = batch*len(X)\n",
    "            avrg_loss = sum_loss/sum_samples\n",
    "            print(f'@train_loop batch={batch:>5d} loss={avrg_loss:>7f} proccesed samples={100*sum_samples/num_samples:>5f}%')\n",
    "    avrg_loss = sum_loss/num_samples\n",
    "    return avrg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd54495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2)\n",
    "# De manera similar, definimos la función de validación\n",
    "def eval_loop(dataloader,model,loss_fn):\n",
    "  # Desactivamos la maquinaria e entrenamiento del modelo\n",
    "  model.eval()\n",
    "  # Definimos ciertas constantes\n",
    "  num_samples = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  sum_loss = 0\n",
    "  sum_samples = 0\n",
    "  # Movemos el modelo a la GPU si es que está disponible\n",
    "  model = model.to(device)\n",
    "  # Para testear, desactivmos el cálculo de gradientes\n",
    "  with torch.no_grad():\n",
    "    # Iteramos sobre lotes (batches)\n",
    "    for X,y in dataloader:\n",
    "      # Copiamos las entradas y salidas al dispositvo de trabajo si es que está disponible\n",
    "      X = X.to(device)\n",
    "      y = y.to(device)\n",
    "      batch_size = len(X)     # number of samples in the batch\n",
    "      sum_samples += batch_size\n",
    "      # Calculamos las predicciones del modelo\n",
    "      pred = model(X)\n",
    "      loss = loss_fn(pred,y)\n",
    "      # Calculamos la pérdida promedio del batch y lo agregamos a una suma correspondiente\n",
    "      sum_loss += loss.item() * batch_size\n",
    "  # Calculamos la pérdida total y la fracción de clasificaciones correctas y las imprimimos\n",
    "  avrg_loss = sum_loss/sum_samples\n",
    "  #print(f'@eval loop avrg loss={avg loss:>8f}')\n",
    "  return avrg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea03544f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_batches=300\n",
      "num_valid_batches=300\n"
     ]
    }
   ],
   "source": [
    "# 4) Preparar Dataset y DataLoader\n",
    "batch_size = 100\n",
    "train_loader_autoencoder = DataLoader(train_set_autoencoder, batch_size=batch_size, shuffle=True)\n",
    "valid_loader_autoencoder = DataLoader(valid_set_autoencoder, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "num_train_batches = len(train_loader_autoencoder)\n",
    "num_valid_batches = len(valid_loader_autoencoder)\n",
    "print(f'num_train_batches={num_train_batches}')\n",
    "print(f'num_valid_batches={num_valid_batches}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5db03285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Definir loops de entrenamiento y validación\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84d51059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6)\n",
    "# Creamos un optimizador, un Stochastic Gradient Descent o un ADAM\n",
    "learning_rate = 1e-3\n",
    "#optimizer = torch.optim.SGD(model.parameter(),lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(),lr=learning_rate,eps=1e-08,weight_decay=0,amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dbe18bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7)\n",
    "# Determinamos en que dispositivo vamos a trabajar, con una CPU o GPU\n",
    "devide = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Pasamos el modelo al dispositivo\n",
    "autoencoder = autoencoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa2cdf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=1.549794 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=1.018086 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.836709 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.768622 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.732786 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.709906 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.692754 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.680117 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.670139 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.661785 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.6555999831358592\n",
      "avg_train_loss.append= 0.5931552676359813\n",
      "avg_valid_loss.append= 0.5929082479079565\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.586677 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.598186 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.594151 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.593491 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.592443 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.592897 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.592992 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.591719 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.591631 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.591369 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5903654382626216\n",
      "avg_train_loss.append= 0.5857706622282663\n",
      "avg_valid_loss.append= 0.5854795004924138\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.589306 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.585832 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.585335 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.586123 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.585975 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.586108 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.586071 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.585923 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.585716 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.585668 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5855563924709956\n",
      "avg_train_loss.append= 0.5823611346880595\n",
      "avg_valid_loss.append= 0.5820428542296092\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.602201 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.587443 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.585177 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.586239 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.584821 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.584251 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.583976 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.583429 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.583385 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.583187 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5831717924276988\n",
      "avg_train_loss.append= 0.5807289346059163\n",
      "avg_valid_loss.append= 0.5804022987683614\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.585673 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.582841 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.584154 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.582089 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.581925 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.581481 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.582320 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.582184 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.582483 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.582101 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5818655878305435\n",
      "avg_train_loss.append= 0.579812581539154\n",
      "avg_valid_loss.append= 0.579484606385231\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.592163 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.585958 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.582450 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.581796 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.581783 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.581020 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.580793 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.581245 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.581011 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.581147 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.58101105093956\n",
      "avg_train_loss.append= 0.5790752242008845\n",
      "avg_valid_loss.append= 0.5787448720137278\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.579819 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.585093 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.582841 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.581653 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.581602 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.581293 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.580666 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.580106 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.580407 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.580338 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5803877973556518\n",
      "avg_train_loss.append= 0.5786230230331421\n",
      "avg_valid_loss.append= 0.57828919450442\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.574030 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.580212 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.580643 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.580607 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.580245 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.579500 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.580058 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.580354 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.580176 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.579823 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5798743083079656\n",
      "avg_train_loss.append= 0.578141701022784\n",
      "avg_valid_loss.append= 0.5778000630935033\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.588604 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.579875 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.578270 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.578392 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.578236 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.578765 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.578925 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.579223 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.579627 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.579533 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5794631113608678\n",
      "avg_train_loss.append= 0.5778113090991974\n",
      "avg_valid_loss.append= 0.5774669375022252\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.576081 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.581465 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.582550 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.581792 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.581340 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.580322 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.579920 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.579907 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.579488 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.579616 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5791015255451203\n",
      "avg_train_loss.append= 0.5774696471293768\n",
      "avg_valid_loss.append= 0.577124674320221\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.576420 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.578721 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.579421 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.579641 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.579161 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.579167 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.578301 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.578568 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.578796 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.578639 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5787734027703603\n",
      "avg_train_loss.append= 0.5772752239306768\n",
      "avg_valid_loss.append= 0.5769298797845841\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.587424 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.582434 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.580655 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.580519 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.579991 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.579260 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.578846 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.579233 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.578582 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.578445 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5784813056389491\n",
      "avg_train_loss.append= 0.5769850862026215\n",
      "avg_valid_loss.append= 0.5766388976573944\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.593164 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.581279 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.578680 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.577592 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.577575 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.578213 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.578000 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.578471 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.578020 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.577784 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5782193160057068\n",
      "avg_train_loss.append= 0.5767954627672831\n",
      "avg_valid_loss.append= 0.576448757648468\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.555308 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.576105 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.579138 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.578748 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.579180 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.578205 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.578153 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.577934 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.577783 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.577709 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5779866550366084\n",
      "avg_train_loss.append= 0.5765914885203044\n",
      "avg_valid_loss.append= 0.5762440198659897\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.587096 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.577347 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.579192 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.578781 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.577460 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.577841 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.578321 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.578232 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.578170 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.578140 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5777687213818232\n",
      "avg_train_loss.append= 0.5763677481810252\n",
      "avg_valid_loss.append= 0.5760193304220835\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.599191 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.582164 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.578737 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.578123 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.577046 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.577012 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.577410 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.576854 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.577221 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.577349 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5775705935557683\n",
      "avg_train_loss.append= 0.5762048621972402\n",
      "avg_valid_loss.append= 0.5758590851227442\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.571843 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.579295 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576899 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.578104 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.579279 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.578234 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.577446 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.577549 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.577871 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.577543 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5773904391129812\n",
      "avg_train_loss.append= 0.576151001850764\n",
      "avg_valid_loss.append= 0.5758011319239934\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.575863 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.579224 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.578924 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.577372 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.577397 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.576927 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.576575 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.576343 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.576440 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.577257 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.577245941956838\n",
      "avg_train_loss.append= 0.5759707321723302\n",
      "avg_valid_loss.append= 0.5756245038906733\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.548547 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.577772 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.574465 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.576042 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.577114 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.576750 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.577564 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.577197 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.576949 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.577306 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5770840164025625\n",
      "avg_train_loss.append= 0.575810596148173\n",
      "avg_valid_loss.append= 0.5754629679520925\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.570920 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.580507 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.577056 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.576979 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576167 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.576453 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.576910 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.577175 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.576618 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.576633 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5769502079486847\n",
      "avg_train_loss.append= 0.5757030393679937\n",
      "avg_valid_loss.append= 0.5753601040442785\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.587601 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.575686 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576690 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.577083 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.577058 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.576848 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.576716 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.576476 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.576842 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.576973 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5768155282735825\n",
      "avg_train_loss.append= 0.5756247939666113\n",
      "avg_valid_loss.append= 0.5752797770500183\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.578252 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.576822 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.575434 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.576146 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576420 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.576850 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.577091 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.577261 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.577310 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.577205 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.576688880721728\n",
      "avg_train_loss.append= 0.5755094848076503\n",
      "avg_valid_loss.append= 0.5751657821734746\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.546092 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.571844 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.573783 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575726 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576242 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575550 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.576060 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.576180 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.576157 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.576579 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5765893801053366\n",
      "avg_train_loss.append= 0.5753792601823807\n",
      "avg_valid_loss.append= 0.5750357115268707\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.565991 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.574375 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.573646 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575706 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574995 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575441 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575931 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.576313 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.576383 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.576219 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5764650801817576\n",
      "avg_train_loss.append= 0.5753038156032563\n",
      "avg_valid_loss.append= 0.574962263305982\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.592330 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.576782 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.575471 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.576379 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576246 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.576028 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.576080 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.576474 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.576290 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.576048 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5763798894484838\n",
      "avg_train_loss.append= 0.5752488921085993\n",
      "avg_valid_loss.append= 0.5749071158965429\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.600527 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.577666 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.575836 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.576004 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.575933 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575245 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.576239 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575891 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.576005 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.576157 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5762782539923986\n",
      "avg_train_loss.append= 0.5751805784304936\n",
      "avg_valid_loss.append= 0.5748378717899323\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.584950 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.572118 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.573727 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.573603 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574691 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575389 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575213 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575295 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575372 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.576221 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5761930062373479\n",
      "avg_train_loss.append= 0.5750596324602762\n",
      "avg_valid_loss.append= 0.5747181671857834\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.556284 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.575824 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.575451 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.576881 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576176 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.576326 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.576261 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575795 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.576338 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.576353 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5761073253552119\n",
      "avg_train_loss.append= 0.575007344285647\n",
      "avg_valid_loss.append= 0.5746667778491974\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.578470 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.578049 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576312 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575629 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576358 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.577406 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.577602 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.576939 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.576413 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575966 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5760364371538162\n",
      "avg_train_loss.append= 0.5749468437830607\n",
      "avg_valid_loss.append= 0.5746039162079494\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.564328 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.574437 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576712 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575792 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.575470 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.576045 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575201 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575783 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.576037 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.576200 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5759483301639556\n",
      "avg_train_loss.append= 0.5748760229349137\n",
      "avg_valid_loss.append= 0.5745339602231979\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.583443 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.576093 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.577246 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.576551 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.575272 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575070 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575697 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575942 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.576071 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.576177 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5758798877398174\n",
      "avg_train_loss.append= 0.5747931315501531\n",
      "avg_valid_loss.append= 0.5744485014677048\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.571385 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.579126 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.577531 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.577589 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576220 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.576963 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.576749 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.576819 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.576181 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.576052 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5758066115776698\n",
      "avg_train_loss.append= 0.5747674183050792\n",
      "avg_valid_loss.append= 0.574427128235499\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.562392 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.577819 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576640 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.577630 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.577014 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.576968 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.576382 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575490 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575934 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575765 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5757633727788926\n",
      "avg_train_loss.append= 0.5747258214155833\n",
      "avg_valid_loss.append= 0.5743829168876012\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.569608 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.582128 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.579456 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.577267 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576543 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575894 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575833 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.576251 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.576153 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575771 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5757132848103841\n",
      "avg_train_loss.append= 0.5746511445442836\n",
      "avg_valid_loss.append= 0.5743095481395721\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.559502 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.575301 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.577799 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.577715 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.577823 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.577657 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.577040 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.577003 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.576303 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.576234 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5756674015522003\n",
      "avg_train_loss.append= 0.574607438047727\n",
      "avg_valid_loss.append= 0.5742678779363632\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.560922 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.576158 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.574775 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.574662 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.575059 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574609 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574616 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574985 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574870 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575372 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.575614088177681\n",
      "avg_train_loss.append= 0.5745635529359182\n",
      "avg_valid_loss.append= 0.5742249262332916\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.565431 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.574807 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.575519 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.573882 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.573905 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574622 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574795 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575478 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575540 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575602 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5755636709928512\n",
      "avg_train_loss.append= 0.5745282516876856\n",
      "avg_valid_loss.append= 0.5741894360383352\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.565059 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.569120 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.571456 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.572269 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.573415 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574107 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575437 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.576158 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.576231 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575971 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5755236140886942\n",
      "avg_train_loss.append= 0.5744856182734172\n",
      "avg_valid_loss.append= 0.5741482265790303\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.571755 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.575195 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.575265 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.577395 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.575562 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575730 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575999 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575801 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574868 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575061 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5754888431231181\n",
      "avg_train_loss.append= 0.5744601196050644\n",
      "avg_valid_loss.append= 0.5741231397787729\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.584610 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.577575 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.575395 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.576254 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.575295 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575019 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575338 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575451 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575467 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575262 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5754591429233551\n",
      "avg_train_loss.append= 0.5744389112790426\n",
      "avg_valid_loss.append= 0.5741006455818812\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.587379 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.579851 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.580321 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.578971 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.577009 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.576747 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.576663 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.576141 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575162 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575009 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5754213525851568\n",
      "avg_train_loss.append= 0.5744355551401774\n",
      "avg_valid_loss.append= 0.5740975205103557\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.578118 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.574267 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.572163 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.574029 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574349 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574458 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574517 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574360 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574503 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575263 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5753884398937226\n",
      "avg_train_loss.append= 0.5743855063120524\n",
      "avg_valid_loss.append= 0.5740490520000457\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.576266 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.575607 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.575712 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.574855 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.575446 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575348 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575613 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575596 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574740 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574864 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5753762664397558\n",
      "avg_train_loss.append= 0.5743998179833094\n",
      "avg_valid_loss.append= 0.5740642795960108\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.584276 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.580007 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.575360 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575036 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576048 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575672 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575805 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575514 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575174 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575081 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5753439450263977\n",
      "avg_train_loss.append= 0.5743732200066248\n",
      "avg_valid_loss.append= 0.5740347184737523\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.586490 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.571247 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.573478 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.573708 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.573904 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574968 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575251 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575729 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575301 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575271 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5753228149811427\n",
      "avg_train_loss.append= 0.5743427302440007\n",
      "avg_valid_loss.append= 0.5740079134702682\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.554815 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.572641 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.573412 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.574897 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.575369 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575650 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575520 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575312 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575247 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575084 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5752899235486985\n",
      "avg_train_loss.append= 0.5743010050058365\n",
      "avg_valid_loss.append= 0.5739644424120585\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.569937 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.580036 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.577470 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.579550 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.577970 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.577078 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.576723 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.576273 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575435 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575105 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5752703430255254\n",
      "avg_train_loss.append= 0.5742782304684321\n",
      "avg_valid_loss.append= 0.5739405608177185\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.567983 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.576403 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576987 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.576108 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.575929 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575965 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575766 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575483 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575739 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575709 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5752473070224127\n",
      "avg_train_loss.append= 0.5742700119813283\n",
      "avg_valid_loss.append= 0.5739347624778748\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.560280 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.571488 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.573548 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.574314 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574548 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574220 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.573517 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574001 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574189 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574808 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5752263188362121\n",
      "avg_train_loss.append= 0.5742533429463704\n",
      "avg_valid_loss.append= 0.5739162196715673\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.567798 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.575391 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.574655 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575550 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.575861 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575709 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575784 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575670 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575471 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575071 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5752085022131602\n",
      "avg_train_loss.append= 0.5742383543650309\n",
      "avg_valid_loss.append= 0.5739030647277832\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.598882 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.576804 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576830 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.576895 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576703 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.576386 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.576092 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575733 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575710 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575638 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5751797195275624\n",
      "avg_train_loss.append= 0.5742158168554305\n",
      "avg_valid_loss.append= 0.5738795576492945\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.573239 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.571459 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.574109 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.574487 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574569 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574065 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574534 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574990 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575056 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575279 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5751588529348374\n",
      "avg_train_loss.append= 0.574186465938886\n",
      "avg_valid_loss.append= 0.5738512216011683\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.570279 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.571887 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.575644 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575026 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574817 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575966 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575765 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575717 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575542 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575444 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5751422278086344\n",
      "avg_train_loss.append= 0.5741810103257498\n",
      "avg_valid_loss.append= 0.5738471968968709\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.582749 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.578016 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576787 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575176 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.575544 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575120 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574711 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574900 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574985 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575036 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.575131427248319\n",
      "avg_train_loss.append= 0.5741572244962057\n",
      "avg_valid_loss.append= 0.57382175664107\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.586917 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.577266 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576364 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575754 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574617 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575105 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575071 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575048 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574851 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574944 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5751076221466065\n",
      "avg_train_loss.append= 0.5741548875967661\n",
      "avg_valid_loss.append= 0.5738204509019852\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.575043 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.577645 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.575195 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575129 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574523 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574158 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574511 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574854 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574962 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574954 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.575101395646731\n",
      "avg_train_loss.append= 0.5741397740443548\n",
      "avg_valid_loss.append= 0.5738075085481008\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.558595 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.570821 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.572191 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.572082 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.573307 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574635 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574924 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575022 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575138 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574901 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5750842424233754\n",
      "avg_train_loss.append= 0.5741253525018692\n",
      "avg_valid_loss.append= 0.5737945034106573\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.586616 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.576827 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576279 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.576795 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576256 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575401 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574869 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575635 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575093 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574696 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5750742969910304\n",
      "avg_train_loss.append= 0.574109477798144\n",
      "avg_valid_loss.append= 0.5737760442495347\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.560431 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.573116 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.574815 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.576032 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576161 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.576415 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575680 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575267 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575616 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575525 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5750539084275563\n",
      "avg_train_loss.append= 0.5741081404685974\n",
      "avg_valid_loss.append= 0.5737758779525757\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.564845 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.573135 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.573744 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.574542 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574472 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575082 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575252 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574853 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575299 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574947 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5750477820634842\n",
      "avg_train_loss.append= 0.574102486371994\n",
      "avg_valid_loss.append= 0.5737696488698324\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.558923 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.575891 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.573749 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.572630 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.573169 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.573714 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574301 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574745 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574599 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574918 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5750294919808706\n",
      "avg_train_loss.append= 0.574061721165975\n",
      "avg_valid_loss.append= 0.5737264859676361\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.563950 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.568699 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.573619 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.574501 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.575123 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574879 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574383 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575036 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575531 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575322 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5750161010026932\n",
      "avg_train_loss.append= 0.5740682617823283\n",
      "avg_valid_loss.append= 0.5737330758571625\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.582588 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.578143 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.578184 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.576628 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576600 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575187 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574894 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575169 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575175 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574818 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5750133490562439\n",
      "avg_train_loss.append= 0.5740689220031102\n",
      "avg_valid_loss.append= 0.5737364725271861\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.561369 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.573752 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.572753 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.573048 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574276 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574861 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574440 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574211 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575171 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574829 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5749965324004491\n",
      "avg_train_loss.append= 0.5740310442447663\n",
      "avg_valid_loss.append= 0.5736976218223572\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.595577 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.575851 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.577602 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.577201 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576291 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.576200 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.576112 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575630 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575441 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575504 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5749963068962097\n",
      "avg_train_loss.append= 0.5740377153952917\n",
      "avg_valid_loss.append= 0.5737048921982447\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.569526 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.573997 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.572671 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.574736 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574479 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575783 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575752 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575545 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575715 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575379 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.574982211391131\n",
      "avg_train_loss.append= 0.5740431121985118\n",
      "avg_valid_loss.append= 0.5737066936492919\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.577833 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.574805 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.574558 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575035 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576125 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575271 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574754 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574996 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575367 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574883 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5749816266695659\n",
      "avg_train_loss.append= 0.574021833539009\n",
      "avg_valid_loss.append= 0.5736889811356862\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.567627 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.575737 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576097 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575559 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576809 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575988 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575506 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574987 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574378 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574747 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5749652077754338\n",
      "avg_train_loss.append= 0.5740247484048208\n",
      "avg_valid_loss.append= 0.573690998951594\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.559945 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.568961 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.570140 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.572548 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.572751 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.573873 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575122 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574423 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574786 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574915 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5749565235773723\n",
      "avg_train_loss.append= 0.5740035132567087\n",
      "avg_valid_loss.append= 0.5736699446042379\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.580376 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.577110 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.574875 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.573386 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574665 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574859 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574763 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574460 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574909 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574742 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.574956218401591\n",
      "avg_train_loss.append= 0.5739920886357626\n",
      "avg_valid_loss.append= 0.5736595171689988\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.592411 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.574821 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576196 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575429 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.575049 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575452 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575279 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575083 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574539 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574368 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5749464565515519\n",
      "avg_train_loss.append= 0.5739955329895019\n",
      "avg_valid_loss.append= 0.5736602280537287\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.557352 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.570715 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.573306 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.574358 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574770 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574395 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575172 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575348 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574754 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575132 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5749412039915721\n",
      "avg_train_loss.append= 0.5739833269516627\n",
      "avg_valid_loss.append= 0.5736495556433996\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.558997 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.574986 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.575795 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575275 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574445 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.573821 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574148 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574561 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574733 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574747 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5749293871720632\n",
      "avg_train_loss.append= 0.5739875364303589\n",
      "avg_valid_loss.append= 0.5736545050144195\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.571198 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.578391 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576955 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.576130 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576171 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575556 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575537 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575471 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575847 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575448 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5749267144997915\n",
      "avg_train_loss.append= 0.573998146255811\n",
      "avg_valid_loss.append= 0.5736636191606521\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.570790 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.577363 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576531 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.574219 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574972 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574387 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574157 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574441 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574406 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574702 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5749160842100779\n",
      "avg_train_loss.append= 0.573971181511879\n",
      "avg_valid_loss.append= 0.573639477690061\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.585720 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.577590 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576498 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.576332 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576177 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.576788 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.576085 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575771 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574994 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575162 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5749148452281951\n",
      "avg_train_loss.append= 0.5739868928988775\n",
      "avg_valid_loss.append= 0.57365431924661\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.574760 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.575295 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576850 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.576621 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576748 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.576316 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575619 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575460 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575242 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575038 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5749113986889521\n",
      "avg_train_loss.append= 0.5740022321542104\n",
      "avg_valid_loss.append= 0.5736695535977682\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.605041 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.576233 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576133 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575819 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.575908 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575312 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575837 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575715 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575727 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575351 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748919057846069\n",
      "avg_train_loss.append= 0.5739586293697357\n",
      "avg_valid_loss.append= 0.5736246353387833\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.583474 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.571048 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.573833 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.574478 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574203 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574431 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574425 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574479 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574810 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574548 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5749004530906677\n",
      "avg_train_loss.append= 0.5739657487471899\n",
      "avg_valid_loss.append= 0.5736309850215912\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.576991 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.574811 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.574835 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575269 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576620 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.576103 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.576163 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.576094 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575602 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575311 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748851203918457\n",
      "avg_train_loss.append= 0.5739763051271438\n",
      "avg_valid_loss.append= 0.57364348868529\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.560942 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.569946 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.573471 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.573191 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.573027 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.573643 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574119 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574830 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574676 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574729 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748805936177571\n",
      "avg_train_loss.append= 0.5739648028214772\n",
      "avg_valid_loss.append= 0.5736328093210856\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.556498 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.572017 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.572728 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.572618 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574221 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574671 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574220 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574299 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574564 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574619 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748712420463562\n",
      "avg_train_loss.append= 0.5739354773362477\n",
      "avg_valid_loss.append= 0.5735997597376505\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.596687 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.577090 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576706 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.576974 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576489 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.576364 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575550 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575295 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575161 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574749 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748723880449931\n",
      "avg_train_loss.append= 0.5739646975199382\n",
      "avg_valid_loss.append= 0.5736307805776596\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.565415 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.579931 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576712 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.576579 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574568 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574424 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574355 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574586 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575091 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574871 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748750879367193\n",
      "avg_train_loss.append= 0.5739326296250026\n",
      "avg_valid_loss.append= 0.5735989320278168\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.556744 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.576278 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.577481 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.577281 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576411 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575421 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575797 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575427 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574748 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574732 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748626279830933\n",
      "avg_train_loss.append= 0.5739298750956853\n",
      "avg_valid_loss.append= 0.5735953958829244\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.578771 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.579357 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.577691 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575838 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.575720 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574990 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574914 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575184 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575080 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574909 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748572645584742\n",
      "avg_train_loss.append= 0.5739522534608841\n",
      "avg_valid_loss.append= 0.5736186675230662\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.550246 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.571866 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.574760 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575884 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.575556 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575325 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575221 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575296 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574980 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574957 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748485159873963\n",
      "avg_train_loss.append= 0.5739663368463517\n",
      "avg_valid_loss.append= 0.5736320396264394\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.584946 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.578916 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576313 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575714 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574745 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574189 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574101 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574863 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574708 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574686 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748483916123708\n",
      "avg_train_loss.append= 0.5739141114552816\n",
      "avg_valid_loss.append= 0.5735797772804896\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.586610 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.579332 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.574689 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575194 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.575350 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575284 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575638 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575077 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575073 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574599 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.574834153453509\n",
      "avg_train_loss.append= 0.5739206598202388\n",
      "avg_valid_loss.append= 0.5735839104652405\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.558637 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.573664 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.575220 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.574347 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574505 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574623 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.573943 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574637 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574779 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574823 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748448173205057\n",
      "avg_train_loss.append= 0.5739095473289489\n",
      "avg_valid_loss.append= 0.5735733596483866\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.602193 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.578218 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576659 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575546 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.575182 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575601 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575687 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575162 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575414 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575041 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748349857330323\n",
      "avg_train_loss.append= 0.5739029181003571\n",
      "avg_valid_loss.append= 0.5735693536202113\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.557364 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.570427 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.572006 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.574634 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574969 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575360 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574296 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574181 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.573910 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574399 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748333152135213\n",
      "avg_train_loss.append= 0.5738939632972081\n",
      "avg_valid_loss.append= 0.573559787273407\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.555371 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.574936 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.574386 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.574443 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574940 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575057 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574859 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574322 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574897 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574619 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748241285483042\n",
      "avg_train_loss.append= 0.5739078654845555\n",
      "avg_valid_loss.append= 0.5735742872953415\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.583808 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.580479 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576228 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.574566 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574895 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575683 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575526 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575518 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575462 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575040 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748255449533463\n",
      "avg_train_loss.append= 0.5739171234766642\n",
      "avg_valid_loss.append= 0.573583389321963\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.583133 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.576337 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.574240 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575314 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.573608 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.573189 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.573723 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574496 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574526 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574830 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748129016160965\n",
      "avg_train_loss.append= 0.5739329038063685\n",
      "avg_valid_loss.append= 0.5735998211304346\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.538870 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.575838 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.575391 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575337 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.575015 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575774 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575125 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575431 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575353 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575135 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748126955827078\n",
      "avg_train_loss.append= 0.5739033907651901\n",
      "avg_valid_loss.append= 0.5735701338450114\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.550475 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.574942 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.574000 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575119 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.574484 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574535 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574579 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575517 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575591 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575310 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748064136505127\n",
      "avg_train_loss.append= 0.5738715314865113\n",
      "avg_valid_loss.append= 0.5735350108146667\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.578772 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.576044 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576617 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.577444 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576357 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.576239 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575413 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575386 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574976 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.575160 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748096718390783\n",
      "avg_train_loss.append= 0.5738868625958761\n",
      "avg_valid_loss.append= 0.5735523704687754\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.580852 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.577445 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.576657 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.575936 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.576561 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.575741 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.575930 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.575624 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.575171 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574914 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748004919290542\n",
      "avg_train_loss.append= 0.5738683354854583\n",
      "avg_valid_loss.append= 0.573532067934672\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "@train_loop batch=    0 loss=0.583587 proccesed samples=0.333333%\n",
      "@train_loop batch=   30 loss=0.572643 proccesed samples=10.333333%\n",
      "@train_loop batch=   60 loss=0.572532 proccesed samples=20.333333%\n",
      "@train_loop batch=   90 loss=0.573189 proccesed samples=30.333333%\n",
      "@train_loop batch=  120 loss=0.573120 proccesed samples=40.333333%\n",
      "@train_loop batch=  150 loss=0.574591 proccesed samples=50.333333%\n",
      "@train_loop batch=  180 loss=0.574620 proccesed samples=60.333333%\n",
      "@train_loop batch=  210 loss=0.574644 proccesed samples=70.333333%\n",
      "@train_loop batch=  240 loss=0.574926 proccesed samples=80.333333%\n",
      "@train_loop batch=  270 loss=0.574930 proccesed samples=90.333333%\n",
      "avg_train_loss_incorrecta.append= 0.5748009985685348\n",
      "avg_train_loss.append= 0.5739184576272964\n",
      "avg_valid_loss.append= 0.5735830507675806\n",
      "Done!\n",
      "Checkpoint guardado en: /home/usuario/Documentos/RedesNeuronales/TPFinal/Clasificadora/1_design/autoencoder_fashionmnist.pt\n"
     ]
    }
   ],
   "source": [
    "# 6) Entrenar Autoencoder y guardar parámetros\n",
    "num_epochs = 100\n",
    "list_avg_train_loss_incorrecta = []\n",
    "list_avg_train_loss = []\n",
    "list_avg_valid_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "  print(f'Epoch {epoch+1}\\n-------------------------------')\n",
    "  avg_train_loss_incorrecta = train_loop(train_loader_autoencoder,autoencoder,loss_fn,optimizer)\n",
    "  avg_train_loss = eval_loop(train_loader_autoencoder,autoencoder,loss_fn)\n",
    "  avg_valid_loss = eval_loop(valid_loader_autoencoder,autoencoder,loss_fn)\n",
    "  list_avg_train_loss_incorrecta.append(avg_train_loss_incorrecta)\n",
    "  list_avg_train_loss.append(avg_train_loss)\n",
    "  list_avg_valid_loss.append(avg_valid_loss)\n",
    "  print('avg_train_loss_incorrecta.append=',avg_train_loss_incorrecta)\n",
    "  print('avg_train_loss.append=',avg_train_loss)\n",
    "  print('avg_valid_loss.append=',avg_valid_loss)\n",
    "print('Done!')\n",
    "\n",
    "# Preparar carpeta de salida (elige la ruta donde guardarás el checkpoint)\n",
    "output_dir = \"/home/usuario/Documentos/RedesNeuronales/TPFinal/Clasificadora/1_design\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "checkpoint = {\n",
    "    \"model_state_dict\": autoencoder.state_dict(),\n",
    "    \"encoder_state_dict\": autoencoder.encoder.state_dict(),\n",
    "    \"hyperparams\": {\n",
    "        \"dropout\": p_dropout,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "    },\n",
    "    \"train_loss_incorreta\": list_avg_train_loss_incorrecta,\n",
    "    \"train_loss\": list_avg_train_loss,\n",
    "    \"valid_loss\": list_avg_valid_loss,\n",
    "    \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "checkpoint_path = os.path.join(output_dir, \"autoencoder_fashionmnist.pt\")\n",
    "\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "print(f\"Checkpoint guardado en: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d664b8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f26a24131f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5.10)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "# plt.xlim(-0.1,5.1)\n",
    "# plt.ylim(-0.1,1.1)\n",
    "plt.plot(list(range(1,len(list_avg_train_loss_incorrecta)+1)),list_avg_train_loss_incorrecta,label='Train loss incorrecta',linestyle='-',c='red')\n",
    "plt.plot(list(range(1,len(list_avg_train_loss)+1)),list_avg_train_loss,label='Train loss',linestyle='-',c='green')\n",
    "plt.plot(list(range(1,len(list_avg_valid_loss)+1)),list_avg_valid_loss,label='Valid loss',linestyle='-',c='blue')\n",
    "plt.title('')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a400c",
   "metadata": {},
   "source": [
    "## Cómo cargar el encoder entrenado desde otro notebook (por ejemplo, `train.ipynb`)\n",
    "\n",
    "Ejemplo de uso en otro archivo:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Asegúrate de tener la misma definición de Autoencoder\n",
    "from autoencoder import Autoencoder  # o copia la clase Autoencoder a tu notebook\n",
    "\n",
    "checkpoint_path = \"/home/usuario/Documentos/RedesNeuronales/TPFinal/Clasificadora/1_design/autoencoder_fashionmnist.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "\n",
    "autoencoder = Autoencoder(dropout=checkpoint[\"hyperparams\"][\"dropout\"])\n",
    "autoencoder.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "encoder = autoencoder.encoder\n",
    "\n",
    "# Ejemplo: pasar encoder a tu modelo Clasificadora\n",
    "clasificador = Clasificadora(p=0.2, n1=128, n2=64, encoder=encoder)\n",
    "```\n",
    "\n",
    "De esta forma puedes entrenar el autoencoder aquí y reutilizar sus parámetros en tu notebook de clasificación (`train.ipynb`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
