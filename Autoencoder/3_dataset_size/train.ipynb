{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRYEofSD0xoF"
      },
      "source": [
        "# PyTorch: Autoencoder convolucional Fashion-MNIST\n",
        "\n",
        "## Refs.\n",
        "\n",
        "* https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
        "\n",
        "* https://github.com/zalandoresearch/fashion-mnist\n",
        "\n",
        "* https://github.com/pranay414/Fashion-MNIST-Pytorch/blob/master/fashion_mnist.ipynb\n",
        "\n",
        "## **Ejercicio 1)** Importando librerías\n",
        "\n",
        "**0)** De ser necesario, **instale PyTorch** escribiendo\n",
        "\n",
        "    !pip3 install torch torchvision torchaudio torchviz\n",
        "\n",
        "**1)** Importe las librerías estandard de Python: `os`, `datetime`, `collections` y `pickle`.\n",
        "\n",
        "**2)** Importe las siguientes librerías third party de Python: `matplotlib.pyplot`, `numpy`, `scipy`, `sklearn`, `pandas`, `dill` y `json`.\n",
        "\n",
        "**3)** Importe las librerias necesarias de **PyTorch**: `torch` y `torchvision`.\n",
        "\n",
        "**4)** Importe la librería: `google.colab`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg3VSqHCGSub",
        "outputId": "23890009-0103-4da4-b063-e05318010f05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (2.9.1)\n",
            "Requirement already satisfied: torchvision in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (0.24.1)\n",
            "Requirement already satisfied: torchaudio in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (2.9.1)\n",
            "Requirement already satisfied: torchviz in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (0.0.3)\n",
            "Requirement already satisfied: filelock in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (2025.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torch) (3.5.1)\n",
            "Requirement already satisfied: numpy in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torchvision) (2.3.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torchvision) (12.0.0)\n",
            "Requirement already satisfied: graphviz in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from torchviz) (0.21)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/usuario/Documentos/RedesNeuronales/TPFinal/.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# 1.0)\n",
        "!pip3 install torch torchvision torchaudio torchviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "I8N3D_nU1_oT"
      },
      "outputs": [],
      "source": [
        "# 1.1)\n",
        "import os\n",
        "import pickle\n",
        "import datetime\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QsfFvPYhkCGl"
      },
      "outputs": [],
      "source": [
        "# 1.2)\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.linalg as linalg\n",
        "import sklearn as skl\n",
        "import pandas as pd\n",
        "#import dill\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Uot5sVNnkCNa"
      },
      "outputs": [],
      "source": [
        "# 1.3)\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.io import read_image\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "#from torchviz import make_dot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oUFvWw_kr7Bt",
        "outputId": "58b6d935-40e6-423f-bd26-5456164539d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcaGEHAd10sb"
      },
      "source": [
        "## **Ejercicio 2)**\n",
        "\n",
        "Bajando y Jugando con el dataset **Fashion-MNIST**.\n",
        "\n",
        "**1)** Baje y transforme los conjuntos de entrenamiento y testeo de FashionMNIST.\n",
        "\n",
        "**2)** Grafique un mosaico de 3x3 imagenes de FashionMNIST, cada una titulada con su respectiva clasificación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "NUoQ9bnwaZ7O",
        "outputId": "df989263-8093-4729-d35b-5d6278de3cc5"
      },
      "outputs": [],
      "source": [
        "# 2.1)\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Download and load the full training data\n",
        "full_dataset = datasets.FashionMNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
        "\n",
        "# Split the full dataset into train and validation sets of (almost) equal size\n",
        "train_size = len(full_dataset) // 2\n",
        "valid_size = len(full_dataset) - train_size\n",
        "train_set_orig, valid_set_orig = random_split(full_dataset, [train_size, valid_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30000 30000\n"
          ]
        }
      ],
      "source": [
        "len(full_dataset)\n",
        "print(train_size, valid_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-wJdl9mKx5EC"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 9 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 2.2)\n",
        "figure = plt.figure()\n",
        "cols,rows = 3,3\n",
        "for i in range(1,cols*rows+1):\n",
        "    j = torch.randint(len(train_set_orig),size=(1,)).item() # Los números aleatorios tambien se pueden generar desde pytorch. Util para trabajar en la GPU.\n",
        "    image,label = train_set_orig[j]\n",
        "    figure.add_subplot(rows,cols,i)\n",
        "    #plt.title(labels_names[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(image.squeeze(),cmap=\"Greys_r\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OWYnfxWz8RS"
      },
      "source": [
        "## Ejercicio 3) Creando un `DataSet` personalizado.\n",
        "\n",
        "**1)** Con el fin de implementar un autoencoder, cree una clase derivada de la clase `DataSet` (llámela, por ejemplo `CustomDataset`) que, en vez de retornal el label asociado a cada imagen de `FashionMNIST`, retorne la imagen misma.\n",
        "\n",
        "**2)** Utilice dicha clase para transformar los conjuntos de entrenamiento y testeo de `FashionMNIST` pensados para clasificación, a correspondientes conjuntos pensados para entrenar un autoencoder.\n",
        "Para ello, defina una clase `CustomDataset` que deriva de la clase `Dataset`, cuyo método `__getitem__(self,i)` retorne el pair `input,output` donde tanto `input` cómo `output` son iguales a la $i$-ésima imagen del dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sPjO1_Av1f87"
      },
      "outputs": [],
      "source": [
        "# 3.1)\n",
        "# Creamos una subclase de la clase Dataset que nos sirva para generar lotes de ejemplos que puedan usarse para entrenar un autoencoder\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self,dataset):\n",
        "        self.dataset=dataset\n",
        "    # Redefinimos el método .__len__()\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    # Redefinimos el método .__getitem__()\n",
        "    def __getitem__(self,i):\n",
        "        image,label=self.dataset[i]\n",
        "        input  = image\n",
        "        output = image #torch.flatten(image) # retornamos la imagen como salida\n",
        "        return input,output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EbMUnqwU19gL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño train_set: 30000\n",
            "Tamaño valid_set: 30000\n"
          ]
        }
      ],
      "source": [
        "# 3.2)\n",
        "# Convertimos FashionMNIST Dataset a CustomDataset\n",
        "train_set = CustomDataset(train_set_orig)\n",
        "valid_set = CustomDataset(valid_set_orig)\n",
        "\n",
        "print(f\"Tamaño train_set: {len(train_set)}\")\n",
        "print(f\"Tamaño valid_set: {len(valid_set)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKpx5sTuPJdk"
      },
      "source": [
        "## Ejercicio 4) Red Neuronal Autoencoder Convolucional\n",
        "\n",
        "**1)** Defina y cree una red neuronal *autoencoder convolucional* constituida por las siguientes capas:\n",
        "\n",
        "\n",
        "1. Capa convolucional 2D (encoder) compuesta por:\n",
        "\n",
        "  * Una capa `Conv2d` (ver [documentación](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)) que mapea una entrada con $1$ canal y dimensiones $(28,28)$ a una salida con $16$ canales y dimensiones $(26,26)$.\n",
        "    Utilice un kernel de dimensiones $(3,3)$ y el resto de los parámetros en sus valores por defecto.\n",
        "  * Una capa `ReLU`.\n",
        "  * Una capa `MaxPool2d` con un kernel de dimensiones $(2,2)$, de modo que la salida resultante tenga dimensiones $(16,13,13)$.\n",
        "  * Una capa `Dropout` con probabilidad $p$.\n",
        "\n",
        "2. Capa lineal (cuello de botella o “bottleneck”) compuesta por:\n",
        "\n",
        "  * Una capa `Flatten` que transforma una entrada de dimensiones $(16,13,13)$ en un vector de dimensión $16\\times 13\\times 13 = 2704$.\n",
        "  * Una capa `Linear` que mapea el vector de dimensión $2704$ a un vector de dimensión $n$ (donde $n$ es un número mucho menor, por ejemplo $n=128$, representando la *codificación comprimida* o *latente*).\n",
        "  * Una capa `ReLU`.\n",
        "  * Una capa `Linear` que mapea de nuevo el vector de dimensión $n$ al espacio original de dimensión $2704$.\n",
        "  * Una capa `ReLU`.\n",
        "\n",
        "3. Capa convolucional 2D transpuesta (decoder) compuesta por:\n",
        "\n",
        "  * Una capa `Unflatten` que mapea el vector de dimensión $2704$ a una representación de $16$ canales con dimensiones $(13,13)$.\n",
        "  * Una capa `ConvTranspose2d` (ver [documentación](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html)) que mapea $16$ canales de dimensiones $(13,13)$ a $1$ canal de dimensiones $(28,28)$. Utilice un kernel de dimensiones $(6,6)$, un stride de $(2,2)$, y un padding de $(1,1)$.\n",
        "  * Una capa `Sigmoid`, para asegurar que las salidas se encuentren en el rango $[0,1]$ (asumiendo que las imágenes originales también fueron normalizadas en dicho rango).\n",
        "\n",
        "**2)** Grafique, a modo de comparación, algunas imágenes de entrada y sus correspondientes reconstrucciones obtenidas con el modelo **sin entrenar** y con una probabilidad de *dropout* $p=0.2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qSJqCozXCEq1"
      },
      "outputs": [],
      "source": [
        "# 4.1)\n",
        "class Autoencoder(nn.Module):\n",
        "    \"\"\"Experimento 1: Autoencoder convolucional básico\"\"\"\n",
        "\n",
        "    def __init__(self, dropout = 0.15):\n",
        "        super().__init__()\n",
        "        # Encoder: (1, 28, 28) -> (32, 14, 14) -> (64, 7, 7)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),  # (1, 28, 28) -> (32, 28, 28)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # (32, 28, 28) -> (32, 14, 14)\n",
        "            nn.Dropout(dropout),\n",
        "            \n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),  # (32, 14, 14) -> (64, 14, 14)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # (64, 14, 14) -> (64, 7, 7)\n",
        "        )\n",
        "\n",
        "        # Decoder: (64, 7, 7) -> (32, 14, 14) -> (1, 28, 28)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=2, stride=2),  # (64, 7, 7) -> (32, 14, 14)\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            nn.ConvTranspose2d(in_channels=32, out_channels=1, kernel_size=2, stride=2),  # (32, 14, 14) -> (1, 28, 28)\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1d3FCZuAzSza"
      },
      "outputs": [],
      "source": [
        "# 4.2)\n",
        "# Creamos el modelo\n",
        "p = 0.15\n",
        "model = Autoencoder(dropout=p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kaN1D9LW2kg-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input size: (image.size())\n",
            "Output size: (output.size())\n"
          ]
        }
      ],
      "source": [
        "# Create a dummy input tensor\n",
        "image = torch.randn(1, 1, 28, 28) # batch size 1, channel 1, size 28x28\n",
        "output = model(image)\n",
        "print(f'Input size: (image.size())')\n",
        "print(f'Output size: (output.size())')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ed-nJILozz4Z"
      },
      "outputs": [],
      "source": [
        "def batch(x):\n",
        "  return x.unsqueeze(0)   # (28,28) -> (1,28,28)\n",
        "\n",
        "def unbatch(x):\n",
        "  return x.squeeze().detach().cpu().numpy() # (1,28,28) -> (28,28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kfr_07W9z-JR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batches_image.size()= torch.Size([1, 1, 28, 28])\n",
            "pred_batched_image.size()= torch.Size([1, 1, 28, 28])\n",
            "pred_image.shape= (28, 28)\n"
          ]
        }
      ],
      "source": [
        "image,_ = train_set[0]\n",
        "batched_image = batch(image)\n",
        "print('batches_image.size()=',batched_image.size())\n",
        "pred_batched_image = model(batched_image)\n",
        "print('pred_batched_image.size()=',pred_batched_image.size())\n",
        "pred_image = unbatch(pred_batched_image)\n",
        "print('pred_image.shape=',pred_image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sWE1i9AC0ZoZ"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Miramos que imagenes predice el modelo sin entrenar\n",
        "figure = plt.figure()\n",
        "rows,cols = 3,2\n",
        "i = 0   #subplot index\n",
        "for row in range(1, rows+1):\n",
        "  j = torch.randint(len(train_set),size=(1,)).item()\n",
        "  # Ploteamos la imagen original\n",
        "  i = i + 1\n",
        "  image,_ = train_set[j]\n",
        "  figure.add_subplot(rows,cols,i)\n",
        "  if row==1:\n",
        "    plt.title('Original')\n",
        "  plt.axis(\"off\")\n",
        "  plt.imshow(unbatch(image),cmap=\"Greys_r\")\n",
        "  # Ploteamos la imagen predicha\n",
        "  i = i + 1\n",
        "  figure.add_subplot(rows,cols,i)\n",
        "  if row==1:\n",
        "    plt.title('predicha')\n",
        "  plt.axis(\"off\")\n",
        "  image_pred = unbatch(model(batch(image)))\n",
        "  plt.imshow(image_pred,cmap=\"Greys_r\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9uINlg69OTw"
      },
      "source": [
        "## Ejercicio 5) Entrenando el modelo\n",
        "\n",
        "**1)** Implemente, en una función, un loop de entrenamiento que recorra los batchs (lotes).\n",
        "\n",
        "**2)** Implemente, en una función, un loop de prueba o validación que recorra los batchs.\n",
        "\n",
        "**3)** Inicialize dos `DataLoader`s llamados `train_loader` y `valid_loader` que estén definidos sobre  el `train_set` (conjunto de entranmiento) y el `valid_set` (conjunto de prueba) de Fashion-MNIST, respectivamente, y que usen batchs de 100 ejemplos.\n",
        "\n",
        "**4)** Cree una función de pérdida usando el **Error Cuadrático Medio**.\n",
        "\n",
        "**5)** Cree un optimizador con un learning rate igual a $10^{-3}$.\n",
        "Pruebe con **ADAM**.\n",
        "\n",
        "**6)** Cree una instancia del modelo con dropout $p=0.2$.\n",
        "\n",
        "**7)** Especifique en que dispositivo (`device`) va a trabajar: en una **CPU** o en una **GPU**.\n",
        "\n",
        "**8)** Implemente un loop que itere sobre épocas de entrenamiento y validación, y que guarde en listas correspondientes los siguientes valores del **ECM**:\n",
        "*  promedios (incorrectos) sobre el conjunto de entrenamiento, calculado **durante** el proceso de entrenamiento sobre la época.\n",
        "*  promedios (correctos) sobre el conjunto de entrenamiento, calculados **posteriormente** al proceso de entrenamiento sobre la época.\n",
        "*  promedios (correctos) sobre el conjunto de validación, calculados **posteriormente** al proceso de entrenamiento sobre la época.\n",
        "\n",
        "**IMPORTANTE:** No olvide copiar los batchs al dispositivo de trabajo.\n",
        "\n",
        "**9)** Entrene y valide el modelo.\n",
        "\n",
        "**10)** Use las listas del inciso **8)** para graficar en función de las **épocas de entrenamiento** el **ECM** de **entrenamiento** y **validación**, respectivamente.\n",
        "Discuta y comente, cual es el número óptimo de épocas de entrenamiento?\n",
        "\n",
        "**11)** Grafique, comparativamente, algunas de las imagenes a predecir vs las imagenes predichas por el modelo entrenado.\n",
        "\n",
        "**12)** Repita para otras elecciones de los hiperparámetros tales como, el optimizador (podría ser el **SGD**), el **learning-rate**, el tamaño de los **batchs**, el **dropout**, **capas convolucionales** y **convolucionales traspuestas** de otros tamaños.\n",
        "En particular, pruebe eliminando, adecuadamente, la **capa lineal**.\n",
        "Que valores de estos hiperparámetros considera los más convenientes? Porqué?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hyuXv-0x29Xw"
      },
      "outputs": [],
      "source": [
        "# 5.1)\n",
        "# Definimos la función de entrenamiento\n",
        "def train_loop(dataloader, model, loss_fn, optimizer,verbose=True):\n",
        "    # Activamos la maquinaria de entrenamiento del modelo\n",
        "    model.train()\n",
        "    # Definimos ciertas constantes\n",
        "    num_samples = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    sum_loss = 0\n",
        "    sum_samples = 0\n",
        "    # Movemos el modelo a la GPU si es que está disponible\n",
        "    model = model.to(device)\n",
        "    #Iteramos sobre lotes (batchs)\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "        # Copiamos las entradas y salidas al dispositvo de trabajo si es que está disponible\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        batch_size = len(X)\n",
        "        sum_samples += batch_size\n",
        "        # Calculamos la predicción del modelo y la correspondiente función de pérdida\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred,y)\n",
        "        # Backpropagamos usando el optimizaor provisto\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Calculamos la pérdida promedio del batch y lo agregamos a una suma correspondiente\n",
        "        sum_loss += loss.item() * batch_size\n",
        "        # Reportamos el progreso\n",
        "        if batch % (num_batches/10) == 0 and verbose:\n",
        "            current = batch*len(X)\n",
        "            avrg_loss = sum_loss/sum_samples\n",
        "            print(f'@train_loop batch={batch:>5d} loss={avrg_loss:>7f} proccesed samples={100*sum_samples/num_samples:>5f}%')\n",
        "    avrg_loss = sum_loss/num_samples\n",
        "    return avrg_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pMc1c_dq4cHz"
      },
      "outputs": [],
      "source": [
        "# 5.2)\n",
        "# De manera similar, definimos la función de validación\n",
        "def eval_loop(dataloader,model,loss_fn):\n",
        "  # Desactivamos la maquinaria e entrenamiento del modelo\n",
        "  model.eval()\n",
        "  # Definimos ciertas constantes\n",
        "  num_samples = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  sum_loss = 0\n",
        "  sum_samples = 0\n",
        "  # Movemos el modelo a la GPU si es que está disponible\n",
        "  model = model.to(device)\n",
        "  # Para testear, desactivmos el cálculo de gradientes\n",
        "  with torch.no_grad():\n",
        "    # Iteramos sobre lotes (batches)\n",
        "    for X,y in dataloader:\n",
        "      # Copiamos las entradas y salidas al dispositvo de trabajo si es que está disponible\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      batch_size = len(X)     # number of samples in the batch\n",
        "      sum_samples += batch_size\n",
        "      # Calculamos las predicciones del modelo\n",
        "      pred = model(X)\n",
        "      loss = loss_fn(pred,y)\n",
        "      # Calculamos la pérdida promedio del batch y lo agregamos a una suma correspondiente\n",
        "      sum_loss += loss.item() * batch_size\n",
        "  # Calculamos la pérdida total y la fracción de clasificaciones correctas y las imprimimos\n",
        "  avrg_loss = sum_loss/sum_samples\n",
        "  #print(f'@eval loop avrg loss={avg loss:>8f}')\n",
        "  return avrg_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "C9_6dFFE5qfl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_train_batches=300\n",
            "num_valid_batches=300\n"
          ]
        }
      ],
      "source": [
        "# 5.3)\n",
        "# Creamos los data loaders\n",
        "batch_size = 100\n",
        "train_loader = DataLoader(train_set,batch_size=batch_size,shuffle=True)\n",
        "valid_loader = DataLoader(valid_set,batch_size=batch_size,shuffle=True)\n",
        "\n",
        "num_train_batches = len(train_loader)\n",
        "num_valid_batches = len(valid_loader)\n",
        "print(f'num_train_batches={num_train_batches}')\n",
        "print(f'num_valid_batches={num_valid_batches}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8YDxkR7p55WL"
      },
      "outputs": [],
      "source": [
        "# 5.4)\n",
        "# Creamos una instancia de una función de pérdida, una entropy loss en este caso\n",
        "#loss_fun = = nn.CrossEntropyLoss() # Para clasificación\n",
        "loss_fn = nn.MSELoss() # Para autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ylGZH0ud6NlR"
      },
      "outputs": [],
      "source": [
        "# 5.5)\n",
        "# Creamos el modelo\n",
        "p = 0.15\n",
        "model = Autoencoder(dropout=p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2rLQoAZq6TSt"
      },
      "outputs": [],
      "source": [
        "# 5.6)\n",
        "# Creamos un optimizador, un Stochastic Gradient Descent o un ADAM\n",
        "learning_rate = 1e-3\n",
        "#optimizer = torch.optim.SGD(model.parameter(),lr=learning_rate)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate,eps=1e-08,weight_decay=0,amsgrad=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Hiqpf176609N"
      },
      "outputs": [],
      "source": [
        "# 5.7)\n",
        "# Determinamos en que dispositivo vamos a trabajar, con una CPU o GPU\n",
        "devide = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Pasamos el modelo al dispositivo\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXhX7KAQCqHk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=1.421256 proccesed samples=0.333333%\n",
            "@train_loop batch=    0 loss=1.421256 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.868735 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.868735 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.758026 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.758026 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.715518 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.715518 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.691412 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.691412 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.676268 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.676268 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.665623 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.665623 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.656601 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.656601 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.649591 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.649591 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.643517 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.643517 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.6388083004951477\n",
            "avg_train_loss.append= 0.5930349260568619\n",
            "avg_valid_loss.append= 0.5939293495814005\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.600574 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.6388083004951477\n",
            "avg_train_loss.append= 0.5930349260568619\n",
            "avg_valid_loss.append= 0.5939293495814005\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.600574 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.595599 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.595599 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.592530 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.592530 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.592297 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.592297 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.592643 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.592643 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.592815 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.592815 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.591556 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.591556 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.591154 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.591154 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.590752 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.590752 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.589992 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.589992 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.589804278810819\n",
            "avg_train_loss.append= 0.5856895973285039\n",
            "avg_valid_loss.append= 0.5866020230452219\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.590323 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.589804278810819\n",
            "avg_train_loss.append= 0.5856895973285039\n",
            "avg_valid_loss.append= 0.5866020230452219\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.590323 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.585930 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.585930 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.587779 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.587779 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.586487 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.586487 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.586300 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.586300 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.585775 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.585775 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.585351 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.585351 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.585555 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.585555 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.585635 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.585635 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.585437 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.585437 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5850958957274754\n",
            "avg_train_loss.append= 0.5821408689022064\n",
            "avg_valid_loss.append= 0.5830631818373998\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.591913 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5850958957274754\n",
            "avg_train_loss.append= 0.5821408689022064\n",
            "avg_valid_loss.append= 0.5830631818373998\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.591913 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.584806 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.584806 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.584168 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.584168 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.583980 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.583980 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.584365 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.584365 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.584317 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.584317 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.583415 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.583415 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.583076 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.583076 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.582858 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.582858 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.582876 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.582876 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.582531380256017\n",
            "avg_train_loss.append= 0.5802306626240412\n",
            "avg_valid_loss.append= 0.5811473019917806\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.575241 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.582531380256017\n",
            "avg_train_loss.append= 0.5802306626240412\n",
            "avg_valid_loss.append= 0.5811473019917806\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.575241 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.578867 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.578867 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.579882 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.579882 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.580632 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.580632 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.580880 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.580880 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.580621 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.580621 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.581295 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.581295 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.581692 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.581692 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.581483 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.581483 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.580956 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.580956 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5810888200998306\n",
            "avg_train_loss.append= 0.5792116250594457\n",
            "avg_valid_loss.append= 0.5801196893056234\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.598251 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5810888200998306\n",
            "avg_train_loss.append= 0.5792116250594457\n",
            "avg_valid_loss.append= 0.5801196893056234\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.598251 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.578310 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.578310 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.576990 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.576990 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.578287 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.578287 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.578906 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.578906 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.579602 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.579602 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.579718 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.579718 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.579447 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.579447 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.580114 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.580114 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.580245 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.580245 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5801859269539515\n",
            "avg_train_loss.append= 0.5784388291835785\n",
            "avg_valid_loss.append= 0.5793387810389201\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.582729 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5801859269539515\n",
            "avg_train_loss.append= 0.5784388291835785\n",
            "avg_valid_loss.append= 0.5793387810389201\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.582729 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.582192 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.582192 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.581789 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.581789 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.580503 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.580503 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.580445 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.580445 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.581075 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.581075 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.580387 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.580387 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.580134 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.580134 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.579771 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.579771 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.579664 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.579664 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5795456117391586\n",
            "avg_train_loss.append= 0.577908210158348\n",
            "avg_valid_loss.append= 0.5788029239575068\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.583542 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5795456117391586\n",
            "avg_train_loss.append= 0.577908210158348\n",
            "avg_valid_loss.append= 0.5788029239575068\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.583542 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.579055 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.579055 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.579580 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.579580 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.578608 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.578608 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.579363 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.579363 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.579576 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.579576 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.579870 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.579870 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.579440 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.579440 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.579297 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.579297 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.579142 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.579142 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.579053836663564\n",
            "avg_train_loss.append= 0.577488181591034\n",
            "avg_valid_loss.append= 0.5783745241165161\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.603059 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.579053836663564\n",
            "avg_train_loss.append= 0.577488181591034\n",
            "avg_valid_loss.append= 0.5783745241165161\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.603059 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.577035 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.577035 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.575295 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.575295 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.577112 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.577112 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.578446 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.578446 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.579074 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.579074 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.578927 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.578927 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.578483 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.578483 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.578311 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.578311 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.578774 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.578774 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5786039382219315\n",
            "avg_train_loss.append= 0.5770768276850382\n",
            "avg_valid_loss.append= 0.5779588981469472\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.571232 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5786039382219315\n",
            "avg_train_loss.append= 0.5770768276850382\n",
            "avg_valid_loss.append= 0.5779588981469472\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.571232 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.578055 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.578055 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.579217 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.579217 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.579394 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.579394 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.578623 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.578623 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.577560 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.577560 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.577327 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.577327 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.577368 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.577368 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.577865 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.577865 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.577929 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.577929 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5782064380248387\n",
            "avg_train_loss.append= 0.5767428225278854\n",
            "avg_valid_loss.append= 0.5776200844844183\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.583119 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5782064380248387\n",
            "avg_train_loss.append= 0.5767428225278854\n",
            "avg_valid_loss.append= 0.5776200844844183\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.583119 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.577519 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.577519 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.574585 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.574585 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.575356 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.575356 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.576485 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.576485 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.577026 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.577026 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.577600 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.577600 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.577340 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.577340 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.577802 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.577802 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.577705 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.577705 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5778752821683883\n",
            "avg_train_loss.append= 0.576437807281812\n",
            "avg_valid_loss.append= 0.5773136758804321\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.594395 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5778752821683883\n",
            "avg_train_loss.append= 0.576437807281812\n",
            "avg_valid_loss.append= 0.5773136758804321\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.594395 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.576325 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.576325 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.576467 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.576467 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.576620 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.576620 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.576298 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.576298 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.576643 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.576643 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.576565 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.576565 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.576482 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.576482 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.576947 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.576947 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.577260 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.577260 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5775679242610932\n",
            "avg_train_loss.append= 0.5761982613801956\n",
            "avg_valid_loss.append= 0.5770700017611186\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.599696 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5775679242610932\n",
            "avg_train_loss.append= 0.5761982613801956\n",
            "avg_valid_loss.append= 0.5770700017611186\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.599696 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.576344 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.576344 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.575565 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.575565 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.574902 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.574902 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.576940 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.576940 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.577090 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.577090 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.577527 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.577527 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.577141 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.577141 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.576734 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.576734 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.577025 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.577025 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5773097860813141\n",
            "avg_train_loss.append= 0.5759563960631688\n",
            "avg_valid_loss.append= 0.5768261482318242\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.575439 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5773097860813141\n",
            "avg_train_loss.append= 0.5759563960631688\n",
            "avg_valid_loss.append= 0.5768261482318242\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.575439 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.577540 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.577540 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.575198 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.575198 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.576112 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.576112 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.577267 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.577267 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.577587 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.577587 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.577003 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.577003 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.577314 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.577314 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.577259 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.577259 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.577339 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.577339 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5770599911610286\n",
            "avg_train_loss.append= 0.5757587780555089\n",
            "avg_valid_loss.append= 0.5766263087590535\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.546129 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5770599911610286\n",
            "avg_train_loss.append= 0.5757587780555089\n",
            "avg_valid_loss.append= 0.5766263087590535\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.546129 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.573632 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.573632 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.575352 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.575352 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.576859 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.576859 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.575551 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.575551 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.576706 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.576706 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.577165 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.577165 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.577048 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.577048 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.576997 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.576997 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.576757 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.576757 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5768489966789881\n",
            "avg_train_loss.append= 0.5755924942096075\n",
            "avg_valid_loss.append= 0.5764604218800863\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.561327 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5768489966789881\n",
            "avg_train_loss.append= 0.5755924942096075\n",
            "avg_valid_loss.append= 0.5764604218800863\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.561327 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.573612 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.573612 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.572177 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.572177 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.573696 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.573696 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.574702 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.574702 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.575448 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.575448 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.575847 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.575847 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.575886 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.575886 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.576321 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.576321 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.576165 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.576165 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5766504679123561\n",
            "avg_train_loss.append= 0.5754053350289663\n",
            "avg_valid_loss.append= 0.5762737842400869\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.594300 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5766504679123561\n",
            "avg_train_loss.append= 0.5754053350289663\n",
            "avg_valid_loss.append= 0.5762737842400869\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.594300 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.574396 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.574396 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.574521 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.574521 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.577146 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.577146 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.577086 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.577086 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.577123 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.577123 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.576670 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.576670 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.576632 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.576632 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.576344 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.576344 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.576471 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.576471 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5764578046401342\n",
            "avg_train_loss.append= 0.5753281418482462\n",
            "avg_valid_loss.append= 0.576195463736852\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.560683 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5764578046401342\n",
            "avg_train_loss.append= 0.5753281418482462\n",
            "avg_valid_loss.append= 0.576195463736852\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.560683 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.573927 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.573927 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.575485 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.575485 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.575757 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.575757 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.576431 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.576431 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.575937 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.575937 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.575962 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.575962 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.576252 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.576252 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.575717 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.575717 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.575950 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.575950 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5762857447067896\n",
            "avg_train_loss.append= 0.575075723528862\n",
            "avg_valid_loss.append= 0.5759375747044881\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.588204 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5762857447067896\n",
            "avg_train_loss.append= 0.575075723528862\n",
            "avg_valid_loss.append= 0.5759375747044881\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.588204 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.578016 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.578016 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.577877 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.577877 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.577069 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.577069 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.576602 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.576602 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.576533 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.576533 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.576370 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.576370 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.575887 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.575887 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.575420 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.575420 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.576009 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.576009 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5761245206991832\n",
            "avg_train_loss.append= 0.5749603195985158\n",
            "avg_valid_loss.append= 0.5758210408687592\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.575465 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5761245206991832\n",
            "avg_train_loss.append= 0.5749603195985158\n",
            "avg_valid_loss.append= 0.5758210408687592\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.575465 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.577083 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.577083 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.576899 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.576899 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.576439 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.576439 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.575390 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.575390 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.575720 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.575720 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.576179 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.576179 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.576351 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.576351 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.576433 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.576433 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.575883 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.575883 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.575986941854159\n",
            "avg_train_loss.append= 0.5748254197835923\n",
            "avg_valid_loss.append= 0.5756865751743316\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.554761 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.575986941854159\n",
            "avg_train_loss.append= 0.5748254197835923\n",
            "avg_valid_loss.append= 0.5756865751743316\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.554761 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.574382 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.574382 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.577134 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.577134 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.578182 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.578182 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.576839 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.576839 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.576535 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.576535 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.576350 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.576350 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.576166 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.576166 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.575896 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.575896 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.575548 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.575548 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5758587127923965\n",
            "avg_train_loss.append= 0.5747275439898173\n",
            "avg_valid_loss.append= 0.5755894180138906\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.559891 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5758587127923965\n",
            "avg_train_loss.append= 0.5747275439898173\n",
            "avg_valid_loss.append= 0.5755894180138906\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.559891 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.575775 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.575775 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.575201 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.575201 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.575412 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.575412 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.574941 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.574941 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.575092 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.575092 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.575329 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.575329 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.575271 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.575271 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.575487 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.575487 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.574930 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.574930 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5757335245609283\n",
            "avg_train_loss.append= 0.5745992561181387\n",
            "avg_valid_loss.append= 0.575461976925532\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.591057 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5757335245609283\n",
            "avg_train_loss.append= 0.5745992561181387\n",
            "avg_valid_loss.append= 0.575461976925532\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.591057 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.579316 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.579316 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.579272 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.579272 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.578302 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.578302 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.577199 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.577199 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.577173 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.577173 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.576734 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.576734 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.576578 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.576578 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.576571 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.576571 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.576199 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.576199 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5756202018260956\n",
            "avg_train_loss.append= 0.5745017101367315\n",
            "avg_valid_loss.append= 0.575362204114596\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.565084 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5756202018260956\n",
            "avg_train_loss.append= 0.5745017101367315\n",
            "avg_valid_loss.append= 0.575362204114596\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.565084 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.575633 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.575633 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.575535 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.575535 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.574171 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.574171 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.575173 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.575173 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.575766 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.575766 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.575292 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.575292 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.575445 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.575445 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.575351 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.575351 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.575678 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.575678 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5755016742149989\n",
            "avg_train_loss.append= 0.5743876840670904\n",
            "avg_valid_loss.append= 0.5752476392189662\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.578393 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5755016742149989\n",
            "avg_train_loss.append= 0.5743876840670904\n",
            "avg_valid_loss.append= 0.5752476392189662\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.578393 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.572644 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.572644 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.575560 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.575560 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.575045 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.575045 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.575326 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.575326 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.575551 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.575551 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.575520 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.575520 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.575205 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.575205 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.575467 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.575467 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.575178 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.575178 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5753902330001195\n",
            "avg_train_loss.append= 0.5742902648448944\n",
            "avg_valid_loss.append= 0.5751539248228074\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.602183 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5753902330001195\n",
            "avg_train_loss.append= 0.5742902648448944\n",
            "avg_valid_loss.append= 0.5751539248228074\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.602183 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.573263 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.573263 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.573516 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.573516 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.574735 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.574735 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.575506 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.575506 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.575773 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.575773 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.576125 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.576125 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.576249 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.576249 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.576009 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.576009 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.575932 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.575932 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5752936949332556\n",
            "avg_train_loss.append= 0.5742489488919577\n",
            "avg_valid_loss.append= 0.575114509065946\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.582240 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5752936949332556\n",
            "avg_train_loss.append= 0.5742489488919577\n",
            "avg_valid_loss.append= 0.575114509065946\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.582240 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.574636 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.574636 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.575630 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.575630 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.575212 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.575212 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.575155 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.575155 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.574685 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.574685 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.574911 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.574911 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.575073 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.575073 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.575431 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.575431 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.575418 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.575418 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5752192628383637\n",
            "avg_train_loss.append= 0.5741567013661066\n",
            "avg_valid_loss.append= 0.5750231834252676\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.575095 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5752192628383637\n",
            "avg_train_loss.append= 0.5741567013661066\n",
            "avg_valid_loss.append= 0.5750231834252676\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.575095 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.573181 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.573181 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.573545 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.573545 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.576051 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.576051 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.575988 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.575988 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.575040 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.575040 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.575212 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.575212 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.575760 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.575760 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.575479 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.575479 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.575348 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.575348 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5751254651943842\n",
            "avg_train_loss.append= 0.5740720019737879\n",
            "avg_valid_loss.append= 0.5749350704749425\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.574771 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5751254651943842\n",
            "avg_train_loss.append= 0.5740720019737879\n",
            "avg_valid_loss.append= 0.5749350704749425\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.574771 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.577718 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.577718 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.575705 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.575705 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.575518 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.575518 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.575450 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.575450 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.575295 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.575295 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.574493 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.574493 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.574946 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.574946 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.575230 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.575230 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.575126 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.575126 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5750595410664876\n",
            "avg_train_loss.append= 0.5740123311678569\n",
            "avg_valid_loss.append= 0.5748782916863759\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.581030 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5750595410664876\n",
            "avg_train_loss.append= 0.5740123311678569\n",
            "avg_valid_loss.append= 0.5748782916863759\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.581030 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.571352 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.571352 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.573871 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.573871 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.575242 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.575242 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.575836 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.575836 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.575143 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.575143 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.574632 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.574632 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.574471 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.574471 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.574621 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.574621 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.574869 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.574869 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5749993010361989\n",
            "avg_train_loss.append= 0.5739601097504298\n",
            "avg_valid_loss.append= 0.5748280302683513\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.589660 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5749993010361989\n",
            "avg_train_loss.append= 0.5739601097504298\n",
            "avg_valid_loss.append= 0.5748280302683513\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.589660 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.574735 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.574735 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.574976 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.574976 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.575282 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.575282 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.574958 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.574958 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.574151 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.574151 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.573916 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.573916 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.573986 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.573986 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.574477 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.574477 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.574603 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.574603 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.574932887951533\n",
            "avg_train_loss.append= 0.5738845950365067\n",
            "avg_valid_loss.append= 0.5747526895999908\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.583525 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.574932887951533\n",
            "avg_train_loss.append= 0.5738845950365067\n",
            "avg_valid_loss.append= 0.5747526895999908\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.583525 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.575118 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.575118 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.575535 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.575535 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.576246 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.576246 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.575996 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.575996 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.575661 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.575661 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.575178 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.575178 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.575307 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.575307 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.575329 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.575329 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.575314 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.575314 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5748722386360169\n",
            "avg_train_loss.append= 0.5738511095444362\n",
            "avg_valid_loss.append= 0.574719291528066\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.567569 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5748722386360169\n",
            "avg_train_loss.append= 0.5738511095444362\n",
            "avg_valid_loss.append= 0.574719291528066\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.567569 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.573198 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.573198 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.571600 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.571600 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.574167 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.574167 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.574372 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.574372 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.574690 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.574690 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.574732 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.574732 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.574767 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.574767 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.574339 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.574339 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.574660 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.574660 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5748257422447205\n",
            "avg_train_loss.append= 0.573804522951444\n",
            "avg_valid_loss.append= 0.5746732550859451\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.584778 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5748257422447205\n",
            "avg_train_loss.append= 0.573804522951444\n",
            "avg_valid_loss.append= 0.5746732550859451\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.584778 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.572311 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.572311 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.573441 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.573441 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.573271 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.573271 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.573167 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.573167 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.573021 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.573021 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.573553 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.573553 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.574378 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.574378 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.574610 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.574610 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.574562 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.574562 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5747707959016164\n",
            "avg_train_loss.append= 0.5737543561061224\n",
            "avg_valid_loss.append= 0.574622718890508\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.583889 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5747707959016164\n",
            "avg_train_loss.append= 0.5737543561061224\n",
            "avg_valid_loss.append= 0.574622718890508\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.583889 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.579552 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.579552 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.576536 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.576536 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.576417 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.576417 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.576094 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.576094 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.576328 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.576328 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.576064 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.576064 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.575042 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.575042 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.575214 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.575214 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.575478 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.575478 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5747300634781519\n",
            "avg_train_loss.append= 0.5737179257472356\n",
            "avg_valid_loss.append= 0.5745859650770823\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.571586 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5747300634781519\n",
            "avg_train_loss.append= 0.5737179257472356\n",
            "avg_valid_loss.append= 0.5745859650770823\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.571586 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.571144 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.571144 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.572573 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.572573 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.574144 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.574144 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.575309 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.575309 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.574932 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.574932 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.574455 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.574455 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.574391 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.574391 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.574926 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.574926 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.574786 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.574786 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5746821453173955\n",
            "avg_train_loss.append= 0.573671918908755\n",
            "avg_valid_loss.append= 0.5745409854253133\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.554537 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5746821453173955\n",
            "avg_train_loss.append= 0.573671918908755\n",
            "avg_valid_loss.append= 0.5745409854253133\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.554537 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.576494 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.576494 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.575090 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.575090 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.573755 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.573755 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.575030 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.575030 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.575722 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.575722 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.575003 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.575003 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.574755 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.574755 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.574956 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.574956 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.575025 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.575025 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5746474572022756\n",
            "avg_train_loss.append= 0.573641210993131\n",
            "avg_valid_loss.append= 0.5745160339275995\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.578974 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5746474572022756\n",
            "avg_train_loss.append= 0.573641210993131\n",
            "avg_valid_loss.append= 0.5745160339275995\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.578974 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.574128 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.574128 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.574989 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.574989 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.574469 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.574469 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.574552 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.574552 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.574876 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.574876 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.574569 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.574569 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.574087 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.574087 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.574183 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.574183 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.574069 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.574069 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.574609721104304\n",
            "avg_train_loss.append= 0.5736159251133601\n",
            "avg_valid_loss.append= 0.5744888627529144\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.575490 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.574609721104304\n",
            "avg_train_loss.append= 0.5736159251133601\n",
            "avg_valid_loss.append= 0.5744888627529144\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.575490 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.574623 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.574623 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.574686 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.574686 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.575348 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.575348 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.574630 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.574630 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.575031 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.575031 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.575397 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.575397 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.574773 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.574773 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.574340 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.574340 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.574675 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.574675 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5745708920558293\n",
            "avg_train_loss.append= 0.5735801756381989\n",
            "avg_valid_loss.append= 0.5744502290089926\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.568321 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5745708920558293\n",
            "avg_train_loss.append= 0.5735801756381989\n",
            "avg_valid_loss.append= 0.5744502290089926\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.568321 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.571930 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.571930 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.575365 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.575365 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.574661 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.574661 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.574415 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.574415 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.574175 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.574175 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.574145 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.574145 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.573920 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.573920 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.574511 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.574511 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.574602 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.574602 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5745303402344386\n",
            "avg_train_loss.append= 0.5735580945014953\n",
            "avg_valid_loss.append= 0.5744276537497839\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.626785 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5745303402344386\n",
            "avg_train_loss.append= 0.5735580945014953\n",
            "avg_valid_loss.append= 0.5744276537497839\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.626785 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.573080 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.573080 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.574813 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.574813 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.575558 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.575558 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.574530 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.574530 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.574001 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.574001 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.574491 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.574491 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.574303 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.574303 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.574721 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.574721 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.574469 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.574469 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5745022960503896\n",
            "avg_train_loss.append= 0.5735227670272192\n",
            "avg_valid_loss.append= 0.5743969448407491\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.585524 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5745022960503896\n",
            "avg_train_loss.append= 0.5735227670272192\n",
            "avg_valid_loss.append= 0.5743969448407491\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.585524 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.573231 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.573231 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.572703 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.572703 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.574217 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.574217 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.573793 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.573793 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.574153 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.574153 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.574408 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.574408 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.574368 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.574368 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.574312 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.574312 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.574424 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.574424 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.574472560485204\n",
            "avg_train_loss.append= 0.5735424826542537\n",
            "avg_valid_loss.append= 0.574418123960495\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.582883 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.574472560485204\n",
            "avg_train_loss.append= 0.5735424826542537\n",
            "avg_valid_loss.append= 0.574418123960495\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.582883 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.573276 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.573276 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.572983 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.572983 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.573749 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.573749 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.574022 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.574022 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.573464 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.573464 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.574505 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.574505 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.574007 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.574007 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.574248 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.574248 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.574428 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.574428 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5744535857439041\n",
            "avg_train_loss.append= 0.5734648523728053\n",
            "avg_valid_loss.append= 0.5743395918607712\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.579724 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5744535857439041\n",
            "avg_train_loss.append= 0.5734648523728053\n",
            "avg_valid_loss.append= 0.5743395918607712\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.579724 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.574520 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.574520 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.573825 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.573825 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.573141 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.573141 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.573314 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.573314 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.573137 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.573137 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.572734 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.572734 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.573760 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.573760 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.574079 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.574079 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.574500 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.574500 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5744292179743449\n",
            "avg_train_loss.append= 0.5734497992197672\n",
            "avg_valid_loss.append= 0.5743243809541067\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.557575 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5744292179743449\n",
            "avg_train_loss.append= 0.5734497992197672\n",
            "avg_valid_loss.append= 0.5743243809541067\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.557575 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.574073 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.574073 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.574115 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.574115 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.572918 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.572918 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.572435 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.572435 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.572906 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.572906 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.573377 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.573377 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.574367 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.574367 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.574352 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.574352 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.574253 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.574253 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5743955785036087\n",
            "avg_train_loss.append= 0.5734272478024165\n",
            "avg_valid_loss.append= 0.5743040307362874\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.584851 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5743955785036087\n",
            "avg_train_loss.append= 0.5734272478024165\n",
            "avg_valid_loss.append= 0.5743040307362874\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.584851 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.575370 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.575370 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.572439 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.572439 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.575214 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.575214 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.576357 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.576357 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.575859 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.575859 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.575130 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.575130 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.575295 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.575295 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.574791 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.574791 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.574669 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.574669 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5743712329864502\n",
            "avg_train_loss.append= 0.573401475350062\n",
            "avg_valid_loss.append= 0.5742772382497787\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.557752 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5743712329864502\n",
            "avg_train_loss.append= 0.573401475350062\n",
            "avg_valid_loss.append= 0.5742772382497787\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.557752 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.573666 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.573666 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.573196 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.573196 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.573448 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.573448 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.574427 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.574427 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.573639 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.573639 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.573712 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.573712 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.574018 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.574018 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.574370 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.574370 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.574444 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.574444 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.574360667069753\n",
            "avg_train_loss.append= 0.5733999677499135\n",
            "avg_valid_loss.append= 0.5742813501755396\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.582521 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.574360667069753\n",
            "avg_train_loss.append= 0.5733999677499135\n",
            "avg_valid_loss.append= 0.5742813501755396\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.582521 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.578000 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.578000 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.575679 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.575679 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.575554 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.575554 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.576052 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.576052 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.574902 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.574902 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.575120 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.575120 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.575500 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.575500 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.575023 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.575023 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.574492 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.574492 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.574339479804039\n",
            "avg_train_loss.append= 0.5733951648076375\n",
            "avg_valid_loss.append= 0.5742737181981404\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.570049 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.574339479804039\n",
            "avg_train_loss.append= 0.5733951648076375\n",
            "avg_valid_loss.append= 0.5742737181981404\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.570049 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.571266 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.571266 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.571738 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.571738 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.571766 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.571766 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.573794 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.573794 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.573044 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.573044 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.573906 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.573906 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.573222 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.573222 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.573733 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.573733 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.574118 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.574118 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.57432479540507\n",
            "avg_train_loss.append= 0.5733678172032038\n",
            "avg_valid_loss.append= 0.5742469141880672\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.544387 proccesed samples=0.333333%\n",
            "avg_train_loss_incorrecta.append= 0.57432479540507\n",
            "avg_train_loss.append= 0.5733678172032038\n",
            "avg_valid_loss.append= 0.5742469141880672\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "@train_loop batch=    0 loss=0.544387 proccesed samples=0.333333%\n",
            "@train_loop batch=   30 loss=0.572282 proccesed samples=10.333333%\n",
            "@train_loop batch=   30 loss=0.572282 proccesed samples=10.333333%\n",
            "@train_loop batch=   60 loss=0.574145 proccesed samples=20.333333%\n",
            "@train_loop batch=   60 loss=0.574145 proccesed samples=20.333333%\n",
            "@train_loop batch=   90 loss=0.574491 proccesed samples=30.333333%\n",
            "@train_loop batch=   90 loss=0.574491 proccesed samples=30.333333%\n",
            "@train_loop batch=  120 loss=0.574644 proccesed samples=40.333333%\n",
            "@train_loop batch=  120 loss=0.574644 proccesed samples=40.333333%\n",
            "@train_loop batch=  150 loss=0.574985 proccesed samples=50.333333%\n",
            "@train_loop batch=  150 loss=0.574985 proccesed samples=50.333333%\n",
            "@train_loop batch=  180 loss=0.574414 proccesed samples=60.333333%\n",
            "@train_loop batch=  180 loss=0.574414 proccesed samples=60.333333%\n",
            "@train_loop batch=  210 loss=0.574460 proccesed samples=70.333333%\n",
            "@train_loop batch=  210 loss=0.574460 proccesed samples=70.333333%\n",
            "@train_loop batch=  240 loss=0.573625 proccesed samples=80.333333%\n",
            "@train_loop batch=  240 loss=0.573625 proccesed samples=80.333333%\n",
            "@train_loop batch=  270 loss=0.574103 proccesed samples=90.333333%\n",
            "@train_loop batch=  270 loss=0.574103 proccesed samples=90.333333%\n",
            "avg_train_loss_incorrecta.append= 0.5743055077393849\n",
            "avg_train_loss.append= 0.573405636548996\n",
            "avg_valid_loss.append= 0.5742818721135458\n",
            "Done!\n",
            "avg_train_loss_incorrecta.append= 0.5743055077393849\n",
            "avg_train_loss.append= 0.573405636548996\n",
            "avg_valid_loss.append= 0.5742818721135458\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# 5.8) y 5.9)\n",
        "# Finalmente, entrenamos iterando sobre épocas\n",
        "# Además, testeamos el modelo en cada una de ellas\n",
        "num_epochs = 50\n",
        "list_avg_train_loss_incorrecta = []\n",
        "list_avg_train_loss = []\n",
        "list_avg_valid_loss = []\n",
        "for epoch in range(num_epochs):\n",
        "  print(f'Epoch {epoch+1}\\n-------------------------------')\n",
        "  avg_train_loss_incorrecta = train_loop(train_loader,model,loss_fn,optimizer)\n",
        "  avg_train_loss = eval_loop(train_loader,model,loss_fn)\n",
        "  avg_valid_loss = eval_loop(valid_loader,model,loss_fn)\n",
        "  list_avg_train_loss_incorrecta.append(avg_train_loss_incorrecta)\n",
        "  list_avg_train_loss.append(avg_train_loss)\n",
        "  list_avg_valid_loss.append(avg_valid_loss)\n",
        "  print('avg_train_loss_incorrecta.append=',avg_train_loss_incorrecta)\n",
        "  print('avg_train_loss.append=',avg_train_loss)\n",
        "  print('avg_valid_loss.append=',avg_valid_loss)\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eje x: epochs\n",
        "s = [i for i in range(1, num_epochs+1)]\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Subplot 1: Train vs Valid\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(s, list_avg_valid_loss, label='Valid Loss (dropout 0.15)', linestyle=\"-\", c='red')\n",
        "plt.plot(s, list_avg_train_loss, label='Train Loss (dropout 0.15)', linestyle=\":\", c='red')\n",
        "\n",
        "plt.title('Pérdidas - Dropout 0.15')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 2: Solo valid\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(s, list_avg_valid_loss, label='Valid Loss (dropout 0.15)', linewidth=2, c='red')\n",
        "\n",
        "plt.title('Validación - Dropout 0.15')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Pérdida final de validación (dropout 0.15):\")\n",
        "print(f\"{list_avg_valid_loss[-1]:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7ferQD9DkuC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x786f0ad4f980>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 5.10)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "# plt.xlim(-0.1,5.1)\n",
        "# plt.ylim(-0.1,1.1)\n",
        "plt.plot(list(range(1,len(list_avg_train_loss_incorrecta)+1)),list_avg_train_loss_incorrecta,label='Train loss incorrecta',linestyle='-',c='red')\n",
        "plt.plot(list(range(1,len(list_avg_train_loss)+1)),list_avg_train_loss,label='Train loss',linestyle='-',c='green')\n",
        "plt.plot(list(range(1,len(list_avg_valid_loss)+1)),list_avg_valid_loss,label='Valid loss',linestyle='-',c='blue')\n",
        "plt.title('')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "19DCw1pt9FxD"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Miramos que imagenes predice el modelo entrenado\n",
        "# Revisar\n",
        "# 5.11)\n",
        "figure = plt.figure()\n",
        "rows,cols = 3,2\n",
        "i = 0 # subplot index\n",
        "model.eval()\n",
        "for row in range(1, rows+1):\n",
        "  j = torch.randint(len(train_set),size=(1,)).item() # Los numeros aleatorios tambien se pueden generar desde pytorch. Util para trabajar en a GPU\n",
        "  # Ploteamos la imagen original\n",
        "  i = i + 1\n",
        "  image,flatten_imagen = train_set[j]\n",
        "  figure.add_subplot(rows,cols,i)\n",
        "  if row==1:\n",
        "    plt.title('Original')\n",
        "  plt.axis(\"off\")\n",
        "  plt.imshow(unbatch(image),cmap=\"Greys_r\")\n",
        "  # Ploteamos la imagen predicha\n",
        "  i = i + 1\n",
        "  figure.add_subplot(rows,cols,i)\n",
        "  if row==1:\n",
        "    plt.title('predicha')\n",
        "  plt.axis(\"off\")\n",
        "  image_pred = unbatch(model(batch(image)))\n",
        "  plt.imshow(image_pred,cmap=\"Greys_r\")\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
